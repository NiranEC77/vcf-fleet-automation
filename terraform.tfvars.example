# VCF Infrastructure Configuration Example
# Based on VCF 9.x JSON Structure
# Copy this file to terraform.tfvars and customize with your values

# ============================================================================
# VCF Provider Configuration
# ============================================================================

# Set to true for INITIAL BOOTSTRAP with Cloud Builder
# Set to false for managing EXISTING VCF with SDDC Manager
use_cloud_builder = true

# Cloud Builder / VCF Installer Configuration (use_cloud_builder = true)
installer_host     = "10.1.1.191"  # Your Cloud Builder hostname/IP
installer_username = "admin@local"
installer_password = "VMware123!VMware123!"

# SDDC Manager Configuration (use_cloud_builder = false)
# Only needed after VCF is deployed
sddc_manager_host     = null  # Set after bootstrap: "sddc-mgr9.vcf.lab"
sddc_manager_username = "admin@local"
sddc_manager_password = null  # Set after bootstrap

allow_unverified_tls = true  # Set to true for self-signed certificates or IP addresses

# ============================================================================
# Management Domain - Basic Configuration
# ============================================================================

deploy_management_domain = true  # Deploy management domain (initial bootstrap only)
instance_id              = "vcf9"
management_pool_name     = "vcf9-network-pool"

skip_esx_thumbprint_validation = true  # Auto-skip thumbprint validation for easier automation
ceip_enabled                   = true
fips_enabled                   = false
vcf_version                    = "9.0.1.0"

# ============================================================================
# DNS and NTP Configuration
# ============================================================================

dns_domain               = "site-a.vcf.lab"
dns_nameserver           = "10.1.1.1"
dns_secondary_nameserver = null
ntp_servers              = ["10.1.1.1"]

# ============================================================================
# Management Domain - vCenter Configuration
# ============================================================================

mgmt_vcenter_hostname       = "vc-mgmt-a.site-a.vcf.lab"
mgmt_vcenter_root_password  = "VMware123!VMware123!"  # Root OS password for vCenter appliance
mgmt_vcenter_admin_password = "VMware123!VMware123!"  # SSO administrator@vsphere.local password
mgmt_vcenter_vm_size        = "medium"
mgmt_vcenter_storage_size   = "lstorage"
mgmt_vcenter_ssl_thumbprint = ""  # Leave empty for automatic - no manual thumbprint needed

# Note: In VCF bootstrap, both passwords are typically set to the same value

# ============================================================================
# Management Domain - Cluster Configuration
# ============================================================================

mgmt_cluster_name    = "vcf9-cl01"
mgmt_datacenter_name = "vcf9-dc01"
mgmt_cluster_evc_mode = null

# ============================================================================
# Management Domain - vSAN Configuration
# ============================================================================

mgmt_vsan_datastore_name       = "vcf9-cl01-ds-vsan01"
mgmt_vsan_failures_to_tolerate = 1
mgmt_vsan_dedup_enabled        = true
mgmt_vsan_esa_enabled          = false

# ============================================================================
# Management Domain - ESXi Hosts
# ============================================================================

mgmt_esxi_hosts = [
  {
    hostname       = "esx-01a.site-a.vcf.lab"
    username       = "root"
    password       = "VMware123!VMware123!"
    ssl_thumbprint = ""  # Leave empty - automatic validation skip
    ssh_thumbprint = ""  # Leave empty - automatic validation skip
  },
  {
    hostname       = "esx-02a.site-a.vcf.lab"
    username       = "root"
    password       = "VMware123!VMware123!"
    ssl_thumbprint = ""
    ssh_thumbprint = ""
  },
  {
    hostname       = "esx-03a.site-a.vcf.lab"
    username       = "root"
    password       = "VMware123!VMware123!"
    ssl_thumbprint = ""
    ssh_thumbprint = ""
  },
  {
    hostname       = "esx-04a.site-a.vcf.lab"
    username       = "root"
    password       = "VMware123!VMware123!"
    ssl_thumbprint = ""
    ssh_thumbprint = ""
  }
]

# ============================================================================
# Management Domain - Network Configuration
# ============================================================================

mgmt_networks = [
  {
    network_type    = "MANAGEMENT"
    vlan_id         = 10
    mtu             = 1500
    subnet          = "10.1.1.0/24"
    subnet_mask     = "255.255.255.0"
    gateway         = "10.1.1.1"
    port_group_key  = "vcf9-cl01-vds01-pg-esx-mgmt"
    teaming_policy  = "loadbalance_loadbased"
    active_uplinks  = ["uplink1", "uplink2"]
    standby_uplinks = []
    ip_ranges       = []
  },
  {
    network_type    = "VMOTION"
    vlan_id         = 13
    mtu             = 9000
    subnet          = "10.1.3.0/25"
    subnet_mask     = "255.255.255.128"
    gateway         = "10.1.3.1"
    port_group_key  = "vcf9-cl01-vds01-pg-vmotion"
    teaming_policy  = "loadbalance_loadbased"
    active_uplinks  = ["uplink1", "uplink2"]
    standby_uplinks = []
    ip_ranges = [
      {
        start = "10.1.3.81"
        end   = "10.1.3.99"
      }
    ]
  },
  {
    network_type    = "VSAN"
    vlan_id         = 11
    mtu             = 9000
    subnet          = "10.1.2.0/25"
    subnet_mask     = "255.255.255.128"
    gateway         = "10.1.2.1"
    port_group_key  = "vcf9-cl01-vds01-pg-vsan"
    teaming_policy  = "loadbalance_loadbased"
    active_uplinks  = ["uplink1", "uplink2"]
    standby_uplinks = []
    ip_ranges = [
      {
        start = "10.1.2.81"
        end   = "10.1.2.99"
      }
    ]
  }
]

# ============================================================================
# Management Domain - DVS Configuration
# ============================================================================

mgmt_dvs_configs = [
  {
    name     = "vcf9-cl01-vds01"
    mtu      = 9000
    networks = ["MANAGEMENT", "VMOTION", "VSAN"]
    vmnic_mappings = [
      {
        vmnic  = "vmnic0"
        uplink = "uplink1"
      },
      {
        vmnic  = "vmnic1"
        uplink = "uplink2"
      }
    ]
    nsx_switch_config = null
    nsx_teamings      = []
  },
  {
    name     = "vcf9-cl01-vds02"
    mtu      = 9000
    networks = []
    vmnic_mappings = [
      {
        vmnic  = "vmnic2"
        uplink = "uplink1"
      },
      {
        vmnic  = "vmnic3"
        uplink = "uplink2"
      }
    ]
    nsx_switch_config = {
      transport_zones = [
        {
          transport_type = "OVERLAY"
          name           = "VCF-Created-Overlay-Zone"
        }
      ]
    }
    nsx_teamings = [
      {
        policy          = "LOADBALANCE_SRCID"
        active_uplinks  = ["uplink1", "uplink2"]
        standby_uplinks = []
      }
    ]
  }
]

# ============================================================================
# Management Domain - NSX Configuration
# ============================================================================

mgmt_nsx_enabled        = true
mgmt_nsx_manager_size   = "medium"
mgmt_nsx_root_password  = "VMware123!VMware123!"
mgmt_nsx_admin_password = "VMware123!VMware123!"
mgmt_nsx_audit_password = "VMware123!VMware123!"
mgmt_nsx_vip_fqdn       = "nsx-mgmt-a.site-a.vcf.lab"
mgmt_nsx_transport_vlan_id = 18

mgmt_nsx_managers = [
  {
    hostname = "nsx-mgmt-01a"
  }
]

# NSX IP Pool for TEP (Tunnel Endpoint)
mgmt_nsx_ip_pool = {
  name        = "vlan-18-tep-network"
  description = "TEP IP Pool for Management Domain"
  subnets = [
    {
      cidr    = "10.1.5.128/25"
      gateway = "10.1.5.129"
      ip_ranges = [
        {
          start = "10.1.5.130"
          end   = "10.1.5.140"
        }
      ]
    }
  ]
}

# ============================================================================
# VCF Automation Configuration
# ============================================================================

vcf_automation_enabled             = true
vcf_automation_hostname            = "auto-a.site-a.vcf.lab"
vcf_automation_ip_pool             = ["10.1.1.70", "10.1.1.71"]
vcf_automation_node_prefix         = "vcfa-appliance"
vcf_automation_internal_cluster_cidr = "198.18.0.0/15"
vcf_automation_admin_password      = "VMware123!VMware123!"

# ============================================================================
# VCF Operations Configuration
# ============================================================================

vcf_operations_enabled           = true
vcf_operations_appliance_size    = "medium"
vcf_operations_admin_password    = "VMware123!VMware123!"
vcf_operations_load_balancer_fqdn = null

vcf_operations_nodes = [
  {
    hostname      = "ops-a.site-a.vcf.lab"
    type          = "master"
    root_password = "VMware123!VMware123!"
  }
]

# ============================================================================
# VCF Operations Collector Configuration
# ============================================================================

vcf_operations_collector_enabled       = true
vcf_operations_collector_hostname      = "opscollector-01a.site-a.vcf.lab"
vcf_operations_collector_appliance_size = "small"
vcf_operations_collector_root_password = "VMware123!VMware123!"

# ============================================================================
# VCF Fleet Manager Configuration
# ============================================================================

vcf_fleet_manager_enabled        = true
vcf_fleet_manager_hostname       = "opslcm-a.site-a.vcf.lab"
vcf_fleet_manager_root_password  = "VMware123!VMware123!"
vcf_fleet_manager_admin_password = "VMware123!VMware123!"

# ============================================================================
# SDDC Manager Additional Configuration (Optional)
# ============================================================================

sddc_manager_config_enabled = true
sddc_manager_hostname       = "sddcmanager-a.site-a.vcf.lab"
sddc_manager_root_password  = "VMware123!VMware123!"
sddc_manager_ssh_password   = "VMware123!VMware123!"
sddc_manager_local_password = "VMware123!VMware123!"

# ============================================================================
# WORKLOAD DOMAIN - Basic Configuration
# ============================================================================

# IMPORTANT: Set to false for initial bootstrap, deploy management domain first
# After management domain is complete, set to true and configure workload domain
workload_domain_enabled  = false
workload_domain_name     = "wld9"
workload_domain_org_name = null

# ============================================================================
# Workload Domain - vCenter Configuration
# ============================================================================

workload_vcenter_name          = "vcsa9-wld"
workload_datacenter_name       = "wld9-DC"
workload_vcenter_root_password = "VMware123!VMware123!"
workload_vcenter_admin_password = "VMware123!VMware123!"  # SSO admin password
workload_vcenter_vm_size       = "medium"
workload_vcenter_storage_size  = "lstorage"
workload_vcenter_ip           = "10.0.0.25"
workload_vcenter_subnet_mask  = "255.255.255.0"
workload_vcenter_gateway      = "10.0.0.221"
workload_vcenter_fqdn         = "vcsa9-wld.vcf.lab"

# ============================================================================
# Workload Domain - SSO Configuration
# ============================================================================

workload_sso_domain_name     = "vcf9-wld.local"
workload_sso_domain_password = "VMware123!VMware123!"

# ============================================================================
# Workload Domain - NSX Configuration
# ============================================================================

workload_nsx_enabled        = true
workload_nsx_admin_password = "VMware123!VMware123!"
workload_nsx_audit_password = "VMware123!VMware123!"
workload_nsx_vip            = "10.0.0.30"
workload_nsx_vip_fqdn       = "nsx9-wld.vcf.lab"
workload_nsx_form_factor    = "large"

workload_nsx_managers = [
  {
    name        = "nsx9-wldappliance1"
    ip_address  = "10.0.0.31"
    fqdn        = "nsx9-wldappliance1.vcf.lab"
    subnet_mask = "255.255.255.0"
    gateway     = "10.0.0.221"
  }
]

# ============================================================================
# Workload Domain - Cluster Configuration
# ============================================================================

workload_clusters = [
  {
    name                      = "cls-wld9"
    cluster_image_id          = "cf715e55-706a-48ab-8838-c6793932811a"
    evc_mode                  = null
    high_availability_enabled = true
    geneve_vlan_id            = 3138

    vsan_datastore = {
      datastore_name                = "cls-wld9-vsan01"
      failures_to_tolerate          = 1
      dedup_and_compression_enabled = true
      esa_enabled                   = false
    }

    # Hosts with their vmnic configurations
    # Host IDs are UUIDs obtained after hosts are commissioned in SDDC Manager
    hosts = [
      {
        id = "e2d0419c-fe99-4a50-92e5-2918e1dfcb3f"
        vmnics = [
          {
            id       = "vmnic0"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic1"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink2"
          },
          {
            id       = "vmnic2"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic3"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink2"
          }
        ]
      },
      {
        id = "1a2ac975-4166-4645-ab93-a5b8c4e3c7cb"
        vmnics = [
          {
            id       = "vmnic0"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic1"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink2"
          },
          {
            id       = "vmnic2"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic3"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink2"
          }
        ]
      },
      {
        id = "061db7f3-e284-4a2f-a99a-ef19179727e8"
        vmnics = [
          {
            id       = "vmnic0"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic1"
            vds_name = "cls-wld9-vds-01"
            uplink   = "uplink2"
          },
          {
            id       = "vmnic2"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink1"
          },
          {
            id       = "vmnic3"
            vds_name = "cls-wld9-vds-02"
            uplink   = "uplink2"
          }
        ]
      }
    ]

    vds_configs = [
      {
        name          = "cls-wld9-vds-01"
        is_used_by_nsx = false
        portgroups = [
          {
            name           = "cls-wld9-vds-01-pg-mgmt"
            transport_type = "MANAGEMENT"
            active_uplinks = ["uplink1", "uplink2"]
          },
          {
            name           = "cls-wld9-vds-01-pg-vmotion"
            transport_type = "VMOTION"
            active_uplinks = ["uplink1", "uplink2"]
          },
          {
            name           = "cls-wld9-vds-01-pg-vsan"
            transport_type = "VSAN"
            active_uplinks = ["uplink1", "uplink2"]
          }
        ]
      },
      {
        name          = "cls-wld9-vds-02"
        is_used_by_nsx = true
        portgroups    = []
      }
    ]

    # IP Pool for NSX TEP in workload domain
    ip_address_pool = {
      name        = "cls-wld9-tep-pool"
      description = "TEP IP Pool for Workload Cluster"
      subnets = [
        {
          cidr    = "10.2.2.0/24"
          gateway = "10.2.2.1"
          ip_ranges = [
            {
              start = "10.2.2.110"
              end   = "10.2.2.130"
            }
          ]
        }
      ]
    }
  }
]

# ============================================================================
# Additional Configuration Notes
# ============================================================================

# 1. Supervisor Configuration:
#    Supervisor is configured via the supervisorActivationSpec in the workload
#    cluster configuration. This is typically done after the workload domain
#    is created. You would add supervisor configuration to the cluster spec.
#
#    Example supervisor IPs from your JSON:
#    - Control Plane IPs: 10.0.0.10 - 10.0.0.15
#    - Service CIDR: 172.12.11.0/24
#    - Private Transit Network CIDR: 173.0.0.0/16
#    - Private CIDR: 174.0.0.0/16

# 2. NSX Edge Nodes:
#    NSX Edge nodes are typically deployed separately using vcf_edge_cluster
#    resource after the workload domain is created.
#
#    Example edge configuration would be added as a separate resource.

# 3. Supervisor Pools:
#    Supervisor pools are configured within the supervisor namespace and
#    are typically managed through the vSphere/NSX interface or separate
#    Terraform resources after supervisor activation.

# 4. License Keys:
#    Set deployWithoutLicenseKeys = true in your configuration, or provide
#    license keys for vSphere, vSAN, and NSX components.

# 5. Host IDs:
#    The host_ids in workload_clusters must be UUIDs from commissioned hosts.
#    You can get these by:
#    - Using the VCF API: GET /v1/hosts
#    - Using Terraform data sources: data "vcf_host"
#    - From the SDDC Manager UI after commissioning hosts
